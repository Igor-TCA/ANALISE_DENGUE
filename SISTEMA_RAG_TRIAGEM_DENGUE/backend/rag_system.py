"""
Sistema RAG (Retrieval-Augmented Generation) para Triagem de Dengue
Utiliza embeddings e LLM para an√°lise de casos cl√≠nicos
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from loguru import logger


class DengueRAGSystem:
    """Sistema RAG para an√°lise e triagem de dengue"""
    
    def __init__(
        self, 
        knowledge_base_path: str,
        vector_store_path: str = "./vectorstore",
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        llm_provider: str = "openai"
    ):
        """
        Inicializa o sistema RAG
        
        Args:
            knowledge_base_path: Caminho para base de conhecimento (JSON)
            vector_store_path: Caminho para salvar/carregar vector store
            embedding_model: Modelo de embeddings
            llm_provider: Provedor LLM (openai ou anthropic)
        """
        load_dotenv()
        
        self.knowledge_base_path = Path(knowledge_base_path)
        self.vector_store_path = Path(vector_store_path)
        self.embedding_model_name = embedding_model
        self.llm_provider = llm_provider
        
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.qa_chain = None
        
        logger.info("Sistema RAG inicializado")
    
    def setup_embeddings(self):
        """Configura modelo de embeddings"""
        logger.info(f"Configurando embeddings: {self.embedding_model_name}")
        
        # Usar embeddings locais (Hugging Face) para economizar API calls
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        logger.info("Embeddings configurados")
        return self
    
    def setup_llm(self, model_name: Optional[str] = None, temperature: float = 0.3):
        """Configura modelo LLM"""
        
        if self.llm_provider == "openai":
            model = model_name or os.getenv("MODEL_NAME", "gpt-4-turbo-preview")
            logger.info(f"Configurando LLM: OpenAI {model}")
            
            self.llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )
        
        elif self.llm_provider == "anthropic":
            from langchain_anthropic import ChatAnthropic
            model = model_name or "claude-3-opus-20240229"
            logger.info(f"Configurando LLM: Anthropic {model}")
            
            self.llm = ChatAnthropic(
                model=model,
                temperature=temperature,
                anthropic_api_key=os.getenv("ANTHROPIC_API_KEY")
            )
        
        else:
            raise ValueError(f"LLM provider n√£o suportado: {self.llm_provider}")
        
        logger.info("LLM configurado")
        return self
    
    def load_and_index_knowledge(self, force_reindex: bool = False):
        """Carrega base de conhecimento e cria/carrega vector store"""
        
        if self.vector_store_path.exists() and not force_reindex:
            logger.info("Carregando vector store existente...")
            self.vectorstore = Chroma(
                persist_directory=str(self.vector_store_path),
                embedding_function=self.embeddings
            )
            logger.info(f"Vector store carregado: {self.vectorstore._collection.count()} documentos")
        
        else:
            logger.info("Criando novo vector store...")
            
            # Carregar base de conhecimento
            with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                knowledge_data = json.load(f)
            
            logger.info(f"Base de conhecimento carregada: {len(knowledge_data)} documentos")
            
            # Converter para documentos LangChain
            documents = self._convert_to_langchain_docs(knowledge_data)
            
            # Criar chunks menores se necess√°rio
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )
            
            split_docs = text_splitter.split_documents(documents)
            logger.info(f"Documentos divididos em {len(split_docs)} chunks")
            
            # Criar vector store
            self.vectorstore = Chroma.from_documents(
                documents=split_docs,
                embedding=self.embeddings,
                persist_directory=str(self.vector_store_path)
            )
            
            self.vectorstore.persist()
            logger.info("Vector store criado e persistido")
        
        return self
    
    def _convert_to_langchain_docs(self, knowledge_data: List[Dict]) -> List[Document]:
        """Converte dados da base de conhecimento para documentos LangChain"""
        documents = []
        
        for item in knowledge_data:
            # Usar narrativa como conte√∫do principal
            content = item.get('texto_narrativo', '')
            
            # Metadados estruturados
            metadata = {
                'tipo': item.get('tipo', 'desconhecido'),
                'id_caso': item.get('id_caso', ''),
            }
            
            # Adicionar metadados espec√≠ficos por tipo
            if item.get('tipo') == 'caso_clinico':
                metadata['faixa_etaria'] = item.get('perfil', {}).get('faixa_etaria', '')
                metadata['classificacao'] = item.get('evolucao', {}).get('classificacao_final', '')
                metadata['desfecho'] = item.get('evolucao', {}).get('desfecho', '')
            
            elif item.get('tipo') == 'padrao_epidemiologico':
                metadata['faixa_etaria'] = item.get('faixa_etaria', '')
                metadata['n_casos'] = item.get('n_casos', 0)
            
            # Enriquecer conte√∫do com informa√ß√£o estruturada
            if item.get('tipo') == 'caso_clinico':
                sintomas = item.get('apresentacao_clinica', {}).get('sintomas', [])
                alarmes = item.get('apresentacao_clinica', {}).get('sinais_alarme', [])
                
                if sintomas:
                    content += f"\nSintomas: {', '.join(sintomas)}"
                if alarmes:
                    content += f"\nSinais de alarme: {', '.join(alarmes)}"
            
            doc = Document(page_content=content, metadata=metadata)
            documents.append(doc)
        
        return documents
    
    def create_qa_chain(self):
        """Cria chain de Question-Answering com contexto m√©dico"""
        
        # Template de prompt especializado para triagem m√©dica
        prompt_template = """Voc√™ √© um sistema especialista em triagem de dengue, treinado com milhares de casos reais do SINAN/DATASUS.

Sua fun√ß√£o √© analisar informa√ß√µes cl√≠nicas de pacientes e determinar o risco de evolu√ß√£o para formas graves de dengue, baseando-se em:
- Padr√µes epidemiol√≥gicos identificados em casos reais
- Fatores de risco conhecidos (idade, comorbidades, sinais de alarme)
- Progress√£o temporal t√≠pica da doen√ßa

Contexto de casos similares da base de dados:
{context}

Informa√ß√µes do paciente atual:
{question}

IMPORTANTE:
- Seja preciso e baseie-se nos dados epidemiol√≥gicos fornecidos
- Identifique sinais de alarme e fatores de risco
- Classifique o risco como: BAIXO, M√âDIO, ALTO ou CR√çTICO
- Forne√ßa recomenda√ß√µes claras de conduta
- Use linguagem t√©cnica mas acess√≠vel para enfermeiros
- Destaque urg√™ncia quando necess√°rio

An√°lise e recomenda√ß√£o:"""

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Criar retriever com busca por similaridade
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # Retornar top 5 documentos mais similares
        )
        
        # Criar chain
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        logger.info("QA Chain criada")
        return self
    
    def analyze_patient(self, patient_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analisa dados de um paciente e retorna avalia√ß√£o de risco
        
        Args:
            patient_data: Dicion√°rio com dados do paciente
            
        Returns:
            Dicion√°rio com an√°lise, classifica√ß√£o de risco e recomenda√ß√µes
        """
        
        # Formatar dados do paciente como query
        query = self._format_patient_query(patient_data)
        
        logger.info(f"Analisando paciente...")
        
        # Executar chain
        result = self.qa_chain({"query": query})
        
        # Extrair informa√ß√µes
        analysis = result['result']
        source_docs = result.get('source_documents', [])
        
        # Classificar risco baseado na resposta
        risk_level = self._extract_risk_level(analysis)
        
        # Montar resposta estruturada
        response = {
            'analysis': analysis,
            'risk_level': risk_level,
            'risk_color': self._get_risk_color(risk_level),
            'similar_cases': self._format_similar_cases(source_docs),
            'confidence': self._calculate_confidence(source_docs),
            'patient_summary': self._create_patient_summary(patient_data)
        }
        
        logger.info(f"An√°lise conclu√≠da - Risco: {risk_level}")
        
        return response
    
    def _format_patient_query(self, patient_data: Dict) -> str:
        """Formata dados do paciente como query para o sistema"""
        
        query_parts = []
        
        # Informa√ß√µes demogr√°ficas
        idade = patient_data.get('idade', 'n√£o informada')
        sexo = patient_data.get('sexo', 'n√£o informado')
        gestante = patient_data.get('gestante', False)
        
        query_parts.append(f"Paciente: {idade} anos, sexo {sexo}")
        if gestante:
            query_parts.append("GESTANTE")
        
        # Dias de sintomas
        dias_sintomas = patient_data.get('dias_sintomas', 0)
        query_parts.append(f"Dias desde in√≠cio dos sintomas: {dias_sintomas}")
        
        # Sintomas
        sintomas = patient_data.get('sintomas', [])
        if sintomas:
            query_parts.append(f"Sintomas presentes: {', '.join(sintomas)}")
        
        # Sinais de alarme
        alarmes = patient_data.get('sinais_alarme', [])
        if alarmes:
            query_parts.append(f"‚ö†Ô∏è SINAIS DE ALARME: {', '.join(alarmes)}")
        
        # Sinais de gravidade
        gravidade = patient_data.get('sinais_gravidade', [])
        if gravidade:
            query_parts.append(f"üö® SINAIS DE GRAVIDADE: {', '.join(gravidade)}")
        
        # Comorbidades
        comorbidades = patient_data.get('comorbidades', [])
        if comorbidades:
            query_parts.append(f"Comorbidades: {', '.join(comorbidades)}")
        
        # Dados laboratoriais
        if 'plaquetas' in patient_data:
            query_parts.append(f"Plaquetas: {patient_data['plaquetas']}/mm¬≥")
        
        if 'hematocrito' in patient_data:
            query_parts.append(f"Hemat√≥crito: {patient_data['hematocrito']}%")
        
        return "\n".join(query_parts)
    
    def _extract_risk_level(self, analysis: str) -> str:
        """Extrai n√≠vel de risco da an√°lise"""
        analysis_upper = analysis.upper()
        
        if 'CR√çTICO' in analysis_upper or 'EMERG√äNCIA' in analysis_upper:
            return 'CR√çTICO'
        elif 'ALTO' in analysis_upper and 'RISCO' in analysis_upper:
            return 'ALTO'
        elif 'M√âDIO' in analysis_upper or 'MODERADO' in analysis_upper:
            return 'M√âDIO'
        else:
            return 'BAIXO'
    
    def _get_risk_color(self, risk_level: str) -> str:
        """Retorna cor associada ao n√≠vel de risco"""
        colors = {
            'BAIXO': 'verde',
            'M√âDIO': 'amarelo',
            'ALTO': 'laranja',
            'CR√çTICO': 'vermelho'
        }
        return colors.get(risk_level, 'cinza')
    
    def _format_similar_cases(self, source_docs: List[Document]) -> List[Dict]:
        """Formata casos similares encontrados"""
        similar = []
        
        for doc in source_docs[:3]:  # Top 3 casos mais similares
            similar.append({
                'content': doc.page_content[:200] + "...",
                'metadata': doc.metadata
            })
        
        return similar
    
    def _calculate_confidence(self, source_docs: List[Document]) -> float:
        """Calcula confian√ßa baseada em documentos recuperados"""
        # Simplificado: baseado no n√∫mero de documentos relevantes encontrados
        if len(source_docs) >= 5:
            return 0.9
        elif len(source_docs) >= 3:
            return 0.75
        elif len(source_docs) >= 1:
            return 0.6
        else:
            return 0.4
    
    def _create_patient_summary(self, patient_data: Dict) -> str:
        """Cria resumo do paciente"""
        idade = patient_data.get('idade', '?')
        sexo = patient_data.get('sexo', '?')
        dias = patient_data.get('dias_sintomas', '?')
        
        n_sintomas = len(patient_data.get('sintomas', []))
        n_alarmes = len(patient_data.get('sinais_alarme', []))
        n_gravidade = len(patient_data.get('sinais_gravidade', []))
        
        summary = f"{idade} anos, {sexo}, {dias} dias de sintomas. "
        summary += f"{n_sintomas} sintomas"
        
        if n_alarmes > 0:
            summary += f", {n_alarmes} sinais de alarme"
        
        if n_gravidade > 0:
            summary += f", {n_gravidade} sinais de gravidade"
        
        return summary
    
    def search_similar_cases(self, query: str, k: int = 5) -> List[Dict]:
        """Busca casos similares na base de conhecimento"""
        
        docs = self.vectorstore.similarity_search(query, k=k)
        
        results = []
        for doc in docs:
            results.append({
                'content': doc.page_content,
                'metadata': doc.metadata
            })
        
        return results
    
    def get_statistics(self) -> Dict:
        """Retorna estat√≠sticas do sistema"""
        
        stats = {
            'total_documents': self.vectorstore._collection.count() if self.vectorstore else 0,
            'embedding_model': self.embedding_model_name,
            'llm_provider': self.llm_provider,
            'vectorstore_path': str(self.vector_store_path)
        }
        
        return stats


def initialize_system(
    knowledge_base_path: str = "./data/knowledge_base.json",
    force_reindex: bool = False
) -> DengueRAGSystem:
    """
    Fun√ß√£o helper para inicializar sistema RAG completo
    
    Args:
        knowledge_base_path: Caminho para base de conhecimento
        force_reindex: Se True, recria vector store
        
    Returns:
        Sistema RAG configurado e pronto para uso
    """
    
    logger.info("Inicializando sistema RAG de triagem de dengue...")
    
    # Criar sistema
    rag_system = DengueRAGSystem(
        knowledge_base_path=knowledge_base_path,
        vector_store_path="./vectorstore",
        embedding_model=os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2"),
        llm_provider=os.getenv("LLM_PROVIDER", "openai")
    )
    
    # Configurar componentes
    rag_system.setup_embeddings() \
              .setup_llm() \
              .load_and_index_knowledge(force_reindex=force_reindex) \
              .create_qa_chain()
    
    logger.info("Sistema RAG inicializado com sucesso!")
    
    return rag_system


if __name__ == "__main__":
    # Teste do sistema
    logger.add("logs/rag_system.log", rotation="10 MB")
    
    # Inicializar
    rag = initialize_system(force_reindex=False)
    
    # Testar com caso exemplo
    test_patient = {
        'idade': 35,
        'sexo': 'F',
        'dias_sintomas': 4,
        'sintomas': ['febre', 'cefaleia', 'mialgia', 'n√°usea'],
        'sinais_alarme': ['vomitos_persistentes', 'dor_abdominal_intensa'],
        'comorbidades': ['hipertensao'],
        'plaquetas': 85000
    }
    
    result = rag.analyze_patient(test_patient)
    
    print("\n=== RESULTADO DA AN√ÅLISE ===")
    print(f"Risco: {result['risk_level']} ({result['risk_color']})")
    print(f"Confian√ßa: {result['confidence']:.0%}")
    print(f"\nResumo: {result['patient_summary']}")
    print(f"\nAn√°lise:\n{result['analysis']}")
